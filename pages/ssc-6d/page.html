<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link rel="stylesheet" type="text/css" href="bootstrap.css">
    <link rel="stylesheet" type="text/css" href="style.css">

    <title>Self-Supervised Category-Level 6D Object Pose Estimation with Deep Implicit Shape Representation</title>
</head>
<body id="body">
    <div id="main">
        <div id="header">
            <div id="title">Self-Supervised Category-Level 6D Object Pose Estimation with Deep Implicit Shape Representation</div>
            <div id="authors">
                <ul class="list-inline">
                    <li class="list-inline-item mx-4">
                        <a href="https://swords123.github.io/">
                            <p>Wanli Peng</p>
                        </a>
                    </li>
                    <li class="list-inline-item mx-4">
                            <p>Jianhang Yan</p>
                    </li>
                    <li class="list-inline-item mx-4">
                        <a href="https://wenht.xyz">
                            <p>Hongtao Wen</p>
                        </a>
                    </li>
                    <li class="list-inline-item mx-4">
                            <p>Yi Sun*</p>
                    </li>
                </ul>
                <ul class="list-inline" style="margin-top: -1rem">
                    <li class="list-inline-item mx-4">
                        <p>Dalian University of Technology</p>
                    </li>
                </ul>
            </div>
            
            <div id="resources">
                <div class="col text-center text_resource">
                    <ul class="list-inline">
                        <li class="list-inline-item mx-4">
                            <a href="https://www.aaai.org/AAAI22Papers/AAAI-3056.PengW.pdf">
                            <img src="paper_icon.png">
                                <p>Paper</p>
                            </a>
                        </li>

                        <li class="list-inline-item mx-4">
                            <a href="https://github.com/swords123/SSC-6D">
                            <img src="github_icon.png">
                                <p>Code</p>
                            </a>
                        </li>

                    </ul>
                </div>
            </div>
        </div>

        <div class="section">
            <h2>Abstract</h2>
            <p>
                Category-level 6D pose estimation can be better generalized to unseen objects in a category compared with instance-
                level 6D pose estimation. However, existing category-level
                6D pose estimation methods usually require supervised training with a sufficient number of 6D pose annotations of objects which makes them difficult to be applied in real scenarios. To address this problem, we propose a self-supervised
                framework for category-level 6D pose estimation in this paper. We leverage DeepSDF as a 3D object representation and
                design several novel loss functions based on DeepSDF to help
                the self-supervised model predict unseen object poses without any 6D object pose labels and explicit 3D models in real
                scenarios. Experiments demonstrate that our method achieves
                comparable performance with the state-of-the-art fully supervised methods on the category-level NOCS benchmark.
            </p>

        </div>

        <div class="section">
            <h2>Overview</h2>
            <div class="text-center">
                <img src="architecture.png" width="95%">
            </div>
            <p>
                Overview of the proposed method. After segmenting individual object instances from the cluttered scene, we build an
                end-to-end network to jointly predict the rotation, translation, scale and shape latent vector of the object. Several self-supervised
                loss functions are designed to constrain consistency between predicted parameters and object observations in real-world data.
            </p>
        </div>

        <div class="section">
            <h2>Results</h2>
            <div class="text-center">
                <img src="results.png" width="93%">
            </div>
            <p>
                Qualitative results on NOCS-REAL test set. The top row shows the estimated pose and size of every object with the
                axis and tight bounding box. The bottom row shows its reconstructed model rendered on the corresponding RGB image
            </p>
        </div>

        <div class="footer">
            The Paper Authors Â© 2022
        </div>

    </div>
</body>
</html>